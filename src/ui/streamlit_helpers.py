import os
import sys
import chromadb
from chromadb.utils import embedding_functions
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Dict, List
import shutil

# Add src to path for imports
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..')))

from src.ingestion.extract import extract_text_from_pdf
from src.ingestion.chunking import chunk_text, save_processed_data
from src.embedding.store import store_embeddings
from src.retrieval.search import semantic_search
from src.retrieval.reranker import rerank_with_llm
from src.utils.logger import setup_logger
from src.utils.error_handler import handle_errors, APIRetryHandler, get_user_friendly_error_message

load_dotenv()

# ãƒ­ã‚¬ãƒ¼ã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
logger = setup_logger("streamlit_helpers")

# APIãƒªãƒˆãƒ©ã‚¤ãƒãƒ³ãƒ‰ãƒ©ãƒ¼
retry_handler = APIRetryHandler(max_retries=3, backoff_factor=2.0)


@handle_errors(logger)
def process_uploaded_pdf(uploaded_file, raw_dir: str = "data/raw",
                        processed_dir: str = "data/processed",
                        storage_path: str = "storage/chroma") -> Dict[str, any]:
    """
    ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸPDFã‚’å‡¦ç†: ä¿å­˜ -> æŠ½å‡º -> ãƒãƒ£ãƒ³ã‚¯åŒ– -> åŸ‹ã‚è¾¼ã¿

    Returns:
        Dict with keys: 'success' (bool), 'message' (str), 'filename' (str), 'chunks_count' (int)
    """
    logger.info(f"PDFãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†é–‹å§‹: {uploaded_file.name}")

    try:
        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºãƒã‚§ãƒƒã‚¯ï¼ˆ10MBåˆ¶é™ï¼‰
        file_size_mb = uploaded_file.size / (1024 * 1024)
        if file_size_mb > 10:
            logger.warning(f"ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºãŒå¤§ãã™ãã¾ã™: {file_size_mb:.2f}MB")
            return {
                'success': False,
                'message': f'ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºãŒå¤§ãã™ãã¾ã™ï¼ˆ{file_size_mb:.2f}MBï¼‰ã€‚10MBä»¥ä¸‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚',
                'filename': uploaded_file.name,
                'chunks_count': 0
            }

        # 1. PDFã‚’ä¿å­˜
        os.makedirs(raw_dir, exist_ok=True)
        os.makedirs("static", exist_ok=True)
        pdf_path = os.path.join(raw_dir, uploaded_file.name)
        static_pdf_path = os.path.join("static", uploaded_file.name)
        
        logger.debug(f"PDFã‚’ä¿å­˜: {pdf_path}")
        with open(pdf_path, "wb") as f:
            f.write(uploaded_file.getbuffer())
        
        # staticãƒ•ã‚©ãƒ«ãƒ€ã«ã‚‚ä¿å­˜ï¼ˆãƒ–ãƒ©ã‚¦ã‚¶é–²è¦§ç”¨ï¼‰
        with open(static_pdf_path, "wb") as f:
            f.write(uploaded_file.getbuffer())

        # 2. ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º
        logger.info("ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºã‚’é–‹å§‹")
        extracted_data = extract_text_from_pdf(pdf_path)
        logger.info(f"ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºå®Œäº†: {len(extracted_data)}ãƒšãƒ¼ã‚¸")

        # 3. ãƒãƒ£ãƒ³ã‚¯åŒ–
        logger.info("ãƒ†ã‚­ã‚¹ãƒˆãƒãƒ£ãƒ³ã‚¯åŒ–ã‚’é–‹å§‹")
        chunks = chunk_text(extracted_data)
        chunks_count = len(chunks)
        logger.info(f"ãƒãƒ£ãƒ³ã‚¯åŒ–å®Œäº†: {chunks_count}ãƒãƒ£ãƒ³ã‚¯")

        # 4. å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
        os.makedirs(processed_dir, exist_ok=True)
        processed_filename = uploaded_file.name.replace('.pdf', '.json')
        processed_path = os.path.join(processed_dir, processed_filename)
        save_processed_data(chunks, processed_path)
        logger.debug(f"å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜: {processed_path}")

        # 5. åŸ‹ã‚è¾¼ã¿ã‚’ä¿å­˜ï¼ˆãƒªãƒˆãƒ©ã‚¤ä»˜ãï¼‰
        logger.info("åŸ‹ã‚è¾¼ã¿ç”Ÿæˆã‚’é–‹å§‹")
        def embed_with_retry():
            return store_embeddings(processed_path, storage_path)

        retry_handler.execute(embed_with_retry)
        logger.info("åŸ‹ã‚è¾¼ã¿ç”Ÿæˆå®Œäº†")

        return {
            'success': True,
            'message': f'PDFã®å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸï¼ {chunks_count}å€‹ã®ãƒãƒ£ãƒ³ã‚¯ã‚’ç”Ÿæˆã—ã¾ã—ãŸã€‚',
            'filename': uploaded_file.name,
            'chunks_count': chunks_count
        }

    except Exception as e:
        return {
            'success': False,
            'message': f'ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}',
            'filename': uploaded_file.name if uploaded_file else None,
            'chunks_count': 0
        }




@handle_errors(logger)
def generate_answer_ui(query: str, storage_path: str = "storage/chroma",
                      n_results: int = 3, initial_k: int = 100, final_k: int = 20) -> Dict[str, any]:
    """
    RAGãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã§ã‚¯ã‚¨ãƒªã«å¯¾ã™ã‚‹å›ç­”ã‚’ç”Ÿæˆï¼ˆUIç”¨ï¼‰

    Args:
        query: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•
        storage_path: ChromaDBã®ä¿å­˜ãƒ‘ã‚¹
        n_results: æœ€çµ‚çš„ã«ä½¿ç”¨ã™ã‚‹ãƒãƒ£ãƒ³ã‚¯æ•°
        initial_k: åˆæœŸå–å¾—ä»¶æ•°
        final_k: ãƒªãƒ©ãƒ³ã‚­ãƒ³ã‚°å¾Œã«æ®‹ã™ä»¶æ•°

    Returns:
        Dict with keys: 'success' (bool), 'answer' (str), 'sources' (List[str]), 'error' (str)
    """
    logger.info(f"ã‚¯ã‚¨ãƒªå‡¦ç†é–‹å§‹: {query[:50]}...")

    try:
        api_key = os.getenv("GOOGLE_API_KEY")
        if not api_key:
            logger.error("API keyãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
            return {
                'success': False,
                'answer': '',
                'sources': [],
                'error': 'GOOGLE_API_KEYãŒ.envãƒ•ã‚¡ã‚¤ãƒ«ã«è¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚'
            }

        # 1. ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ (Retrieval) - åºƒã‚ã«å–å¾—
        logger.info(f"ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢é–‹å§‹: åˆæœŸå–å¾—{initial_k}ä»¶")
        initial_results = semantic_search(query, storage_path, top_k=initial_k)
        logger.info(f"ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢å®Œäº†: {len(initial_results)}ä»¶ã®ãƒãƒ£ãƒ³ã‚¯ã‚’å–å¾—")

        # 2. LLMãƒªãƒ©ãƒ³ã‚­ãƒ³ã‚°
        logger.info(f"LLMãƒªãƒ©ãƒ³ã‚­ãƒ³ã‚°é–‹å§‹: ä¸Šä½{final_k}ä»¶ã«çµã‚Šè¾¼ã¿")
        reranked_results = rerank_with_llm(query, initial_results, top_k=final_k)
        logger.info(f"ãƒªãƒ©ãƒ³ã‚­ãƒ³ã‚°å®Œäº†: {len(reranked_results)}ä»¶")

        # 3. ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ§‹ç¯‰ã¨ã‚½ãƒ¼ã‚¹æƒ…å ±ã®æ•´ç†
        context = ""
        page_sources = {}  # ãƒšãƒ¼ã‚¸ã”ã¨ã«ãƒãƒ£ãƒ³ã‚¯ã‚’ã‚°ãƒ«ãƒ¼ãƒ—åŒ–

        # æœ€çµ‚çš„ã«n_resultsä»¶ã®ã¿ä½¿ç”¨
        for i, result in enumerate(reranked_results[:n_results]):
            chunk_text = result['content']
            distance = result.get('distance', 0)
            rerank_score = result.get('rerank_score', 0)

            context += f"\n--- è³‡æ–™ {i+1} ---\n{chunk_text}\n"

            page = result['metadata'].get('page', 'ä¸æ˜')
            source = result['metadata'].get('source', 'ä¸æ˜')

            # ãƒšãƒ¼ã‚¸ã”ã¨ã«ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
            page_key = f"{source}_{page}"
            if page_key not in page_sources:
                page_sources[page_key] = {
                    'page': page,
                    'source': source,
                    'chunks': [],
                    'avg_distance': 0,
                    'avg_rerank_score': 0,
                    'count': 0
                }
            # ãƒãƒ£ãƒ³ã‚¯ã®æœ€åˆã®100æ–‡å­—ã‚’ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ã¨ã—ã¦ä¿å­˜
            preview = chunk_text[:100].replace('\n', ' ') + "..." if len(chunk_text) > 100 else chunk_text
            page_sources[page_key]['chunks'].append(preview)
            page_sources[page_key]['avg_distance'] += distance
            page_sources[page_key]['avg_rerank_score'] += rerank_score
            page_sources[page_key]['count'] += 1

        # å¹³å‡ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—ã—ã¦ã‚½ãƒ¼ãƒˆï¼ˆãƒªãƒ©ãƒ³ã‚¯ã‚¹ã‚³ã‚¢ã§é™é †ï¼‰
        for page_key in page_sources:
            page_sources[page_key]['avg_distance'] /= page_sources[page_key]['count']
            page_sources[page_key]['avg_rerank_score'] /= page_sources[page_key]['count']

        sorted_pages = sorted(page_sources.items(),
                            key=lambda x: x[1]['avg_rerank_score'],
                            reverse=True)
        
        # ã‚½ãƒ¼ã‚¹æƒ…å ±ã‚’ã‚¿ãƒ—ãƒ«å½¢å¼ã§ç”Ÿæˆï¼ˆãƒšãƒ¼ã‚¸ã”ã¨ã«1ã¤ã€é–¢é€£åº¦é †ï¼‰
        sources = []
        for page_key, info in sorted_pages:
            page = info['page']
            source = info['source']
            chunk_count = len(info['chunks'])
            url = f"http://localhost:8503/{source}#page={page}"
            text = f"ğŸ“„ ãƒšãƒ¼ã‚¸ {page} ({source}) - {chunk_count}ä»¶"
            # ã‚¿ãƒ—ãƒ«: (page, source, url, text, chunks_preview)
            # chunksã‚‚ã‚¿ãƒ—ãƒ«ã«å¤‰æ›ï¼ˆStreamlitã®ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã«å¯¾å¿œï¼‰
            sources.append((page, source, url, text, tuple(info['chunks'])))

        prompt = f"""
ã‚ãªãŸã¯æä¾›ã•ã‚ŒãŸè³‡æ–™ã«åŸºã¥ã„ã¦è³ªå•ã«ç­”ãˆã‚‹ã€èª å®Ÿã§å½¹ç«‹ã¤ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚

ã€é‡è¦ãªæŒ‡ç¤ºã€‘
1. ä»¥ä¸‹ã®è³‡æ–™ã‚’**ã™ã¹ã¦æ³¨æ„æ·±ãèª­ã¿**ã€è³ªå•ã«é–¢é€£ã™ã‚‹æƒ…å ±ã‚’æ¢ã—ã¦ãã ã•ã„
2. è³ªå•ã«ç›´æ¥ç­”ãˆã¦ã„ã‚‹ç®‡æ‰€ã ã‘ã§ãªãã€**é–¢é€£ã™ã‚‹æ–‡è„ˆã‚„èƒŒæ™¯æƒ…å ±**ã‚‚å«ã‚ã¦å›ç­”ã—ã¦ãã ã•ã„
3. è¤‡æ•°ã®è³‡æ–™ã«é–¢é€£æƒ…å ±ãŒã‚ã‚‹å ´åˆã¯ã€ãã‚Œã‚‰ã‚’**çµ±åˆã—ã¦åŒ…æ‹¬çš„ãªå›ç­”**ã‚’ä½œæˆã—ã¦ãã ã•ã„
4. å›ç­”ã®æ ¹æ‹ ã¨ãªã‚‹è³‡æ–™ç•ªå·ã‚’æ˜è¨˜ã—ã¦ãã ã•ã„ï¼ˆä¾‹: [è³‡æ–™ 1, 3]ï¼‰
5. è³‡æ–™ã«æƒ…å ±ãŒå«ã¾ã‚Œã¦ã„ã‚‹å ´åˆã¯ã€å¿…ãšå…·ä½“çš„ã«ç­”ãˆã¦ãã ã•ã„
6. æœ¬å½“ã«æƒ…å ±ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã®ã¿ã€Œæä¾›ã•ã‚ŒãŸè³‡æ–™ã«ã¯ãã®æƒ…å ±ãŒå«ã¾ã‚Œã¦ã„ã¾ã›ã‚“ã€ã¨ç­”ãˆã¦ãã ã•ã„

ã€è³‡æ–™ã€‘
{context}

ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã€‘
{query}

ã€å›ç­”ã€‘
ä¸Šè¨˜ã®è³‡æ–™ã«åŸºã¥ã„ã¦ã€è³ªå•ã«å¯¾ã™ã‚‹è©³ã—ã„å›ç­”ã‚’æ—¥æœ¬èªã§è¨˜è¿°ã—ã¦ãã ã•ã„ã€‚
"""

        # 3. ç”Ÿæˆ (Generation) - ãƒªãƒˆãƒ©ã‚¤ä»˜ã
        logger.info("å›ç­”ç”Ÿæˆé–‹å§‹")

        def generate_with_retry():
            genai.configure(api_key=api_key)
            model = genai.GenerativeModel('models/gemini-flash-latest')
            return model.generate_content(prompt)

        response = retry_handler.execute(generate_with_retry)
        logger.info(f"å›ç­”ç”Ÿæˆå®Œäº†: {len(response.text)}æ–‡å­—")

        return {
            'success': True,
            'answer': response.text,
            'sources': list(dict.fromkeys(sources)),  # é †åºã‚’ç¶­æŒã—ã¦é‡è¤‡é™¤å»
            'error': ''
        }

    except Exception as e:
        logger.error(f"å›ç­”ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
        user_message = get_user_friendly_error_message(e)
        return {
            'success': False,
            'answer': '',
            'sources': [],
            'error': user_message
        }


def format_sources(sources: List[str]) -> str:
    """
    ã‚½ãƒ¼ã‚¹ãƒªã‚¹ãƒˆã‚’ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã—ã¦æ–‡å­—åˆ—ã¨ã—ã¦è¿”ã™
    """
    if not sources:
        return "ã‚½ãƒ¼ã‚¹ãªã—"

    return "\n".join([f"â€¢ {source}" for source in sources])


def clear_chat_history(session_state) -> None:
    """
    ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã‚’ã‚¯ãƒªã‚¢
    """
    if 'messages' in session_state:
        session_state.messages = []


def check_db_status(storage_path: str = "storage/chroma") -> Dict[str, any]:
    """
    ChromaDBã®ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚’ç¢ºèª

    Returns:
        Dict with keys: 'exists' (bool), 'document_count' (int), 'collections' (List[str])
    """
    try:
        if not os.path.exists(storage_path):
            return {
                'exists': False,
                'document_count': 0,
                'collections': []
            }

        api_key = os.getenv("GOOGLE_API_KEY")
        if not api_key:
            return {
                'exists': False,
                'document_count': 0,
                'collections': [],
                'error': 'API key not configured'
            }

        client = chromadb.PersistentClient(path=storage_path)
        gemini_ef = embedding_functions.GoogleGenerativeAiEmbeddingFunction(
            api_key=api_key,
            model_name="models/text-embedding-004",
            task_type="RETRIEVAL_QUERY"
        )

        try:
            collection = client.get_collection(
                name="notebook_rag_collection",
                embedding_function=gemini_ef
            )
            count = collection.count()
            return {
                'exists': True,
                'document_count': count,
                'collections': ['notebook_rag_collection']
            }
        except Exception:
            return {
                'exists': True,
                'document_count': 0,
                'collections': []
            }

    except Exception as e:
        return {
            'exists': False,
            'document_count': 0,
            'collections': [],
            'error': str(e)
        }


def clear_database(storage_path: str = "storage/chroma") -> Dict[str, any]:
    """
    ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’ã‚¯ãƒªã‚¢

    Args:
        storage_path: Chromaãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®ãƒ‘ã‚¹

    Returns:
        å‡¦ç†çµæœã®è¾æ›¸
    """
    import shutil

    try:
        if os.path.exists(storage_path):
            shutil.rmtree(storage_path)
            logger.info("ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’ã‚¯ãƒªã‚¢ã—ã¾ã—ãŸ")
            return {
                'success': True,
                'message': 'âœ… ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’ã‚¯ãƒªã‚¢ã—ã¾ã—ãŸã€‚ãƒšãƒ¼ã‚¸ã‚’ãƒªãƒ­ãƒ¼ãƒ‰ï¼ˆF5ï¼‰ã—ã¦ã‹ã‚‰ã€PDFã‚’å†ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚'
            }
        else:
            return {
                'success': True,
                'message': 'ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã¯æ—¢ã«ç©ºã§ã™'
            }
    except PermissionError as e:
        logger.error(f"ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚¯ãƒªã‚¢ã‚¨ãƒ©ãƒ¼ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ãƒ­ãƒƒã‚¯ï¼‰: {e}")
        return {
            'success': False,
            'message': '''âš ï¸ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®ã‚¯ãƒªã‚¢ã«å¤±æ•—ã—ã¾ã—ãŸ

**åŸå› **: åˆ¥ã®ãƒ—ãƒ­ã‚»ã‚¹ãŒãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨ä¸­ã§ã™ã€‚

**è§£æ±ºæ–¹æ³•**:
ã™ã¹ã¦ã®ã‚¿ãƒ¼ãƒŸãƒŠãƒ«ã§ Ctrl+C ã‚’æŠ¼ã—ã¦ãƒ—ãƒ­ã‚»ã‚¹ã‚’åœæ­¢ã—ã¦ã‹ã‚‰ã€ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã‚’å®Ÿè¡Œ:

```powershell
powershell -ExecutionPolicy Bypass -File .\\cleanup_and_restart.ps1
```
'''
        }
    except Exception as e:
        logger.error(f"ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚¯ãƒªã‚¢ã‚¨ãƒ©ãƒ¼: {e}")
        return {
            'success': False,
            'message': f'ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}'
        }


def process_multiple_pdfs(uploaded_files: List, raw_dir: str = "data/raw",
                         processed_dir: str = "data/processed",
                         storage_path: str = "storage/chroma") -> Dict[str, any]:
    """
    è¤‡æ•°ã®PDFãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¸€æ‹¬å‡¦ç†

    Returns:
        Dict with keys: 'success' (bool), 'message' (str), 'results' (List[Dict]), 'total_chunks' (int)
    """
    logger.info(f"è¤‡æ•°PDFå‡¦ç†é–‹å§‹: {len(uploaded_files)}ãƒ•ã‚¡ã‚¤ãƒ«")

    results = []
    total_chunks = 0
    failed_files = []

    for uploaded_file in uploaded_files:
        result = process_uploaded_pdf(uploaded_file, raw_dir, processed_dir, storage_path)
        results.append({
            'filename': uploaded_file.name,
            'success': result['success'],
            'message': result['message'],
            'chunks_count': result['chunks_count']
        })

        if result['success']:
            total_chunks += result['chunks_count']
        else:
            failed_files.append(uploaded_file.name)

    success_count = len([r for r in results if r['success']])
    message = f"{success_count}/{len(uploaded_files)}ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ­£å¸¸ã«å‡¦ç†ã—ã¾ã—ãŸã€‚åˆè¨ˆ{total_chunks}ãƒãƒ£ãƒ³ã‚¯ã€‚"

    if failed_files:
        message += f"\nå¤±æ•—: {', '.join(failed_files)}"

    logger.info(f"è¤‡æ•°PDFå‡¦ç†å®Œäº†: {message}")

    return {
        'success': success_count > 0,
        'message': message,
        'results': results,
        'total_chunks': total_chunks
    }


def get_processed_pdfs(raw_dir: str = "data/raw") -> List[str]:
    """
    å‡¦ç†æ¸ˆã¿ã®PDFãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§ã‚’å–å¾—

    Returns:
        PDFãƒ•ã‚¡ã‚¤ãƒ«åã®ãƒªã‚¹ãƒˆ
    """
    if not os.path.exists(raw_dir):
        return []

    pdf_files = [f for f in os.listdir(raw_dir) if f.endswith('.pdf')]
    return sorted(pdf_files)
